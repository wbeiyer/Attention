nohup: ignoring input
----------------- Options ---------------
               batch_size: 16                            	[default: 1]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 256                           
                 dataroot: ./datasets/thread             	[default: None]
             dataset_mode: unaligned                     
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 10                            
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: lsgan                         
                  gpu_ids: 0,1,2,3,4,5,6,7               	[default: 0]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                 lambda_A: 10.0                          
                 lambda_B: 10.0                          
          lambda_identity: 0.5                           
                load_iter: 0                             	[default: 0]
                load_size: 286                           
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: attention_gan                 	[default: cycle_gan]
               n_layers_D: 3                             
                     name: thread_attentiongan           	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: resnet_9blocks                
                      ngf: 64                            
                    niter: 100                           
              niter_decay: 100                           
               no_dropout: True                          
                  no_flip: False                         
                  no_html: False                         
                     norm: instance                      
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 50                            
               preprocess: resize_and_crop               
               print_freq: 100                           
                 saveDisk: False                         
             save_by_iter: False                         
          save_epoch_freq: 10                            
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                  verbose: False                         
----------------- End -------------------
dataset [UnalignedDataset] was created
The number of training images = 16224
initialize network with normal
initialize network with normal
initialize network with normal
initialize network with normal
model [AttentionGANModel] was created
---------- Networks initialized -------------
[Network G_A] Total number of parameters : 11.823 M
[Network G_B] Total number of parameters : 11.823 M
[Network D_A] Total number of parameters : 2.765 M
[Network D_B] Total number of parameters : 2.765 M
-----------------------------------------------
End of epoch 1 / 200 	 
learning rate = 0.0002000
End of epoch 2 / 200 	 
learning rate = 0.0002000
End of epoch 3 / 200 	 
learning rate = 0.0002000
End of epoch 4 / 200 	 
learning rate = 0.0002000
End of epoch 5 / 200 	 
learning rate = 0.0002000
End of epoch 6 / 200 	 
learning rate = 0.0002000
End of epoch 7 / 200 	 
learning rate = 0.0002000
End of epoch 8 / 200 	 
learning rate = 0.0002000
End of epoch 9 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 10, iters 162240
D_A: 0.010257335701182162
G_A: 0.0329672020958292
cycle_A: 0.010940413338846768
idt_A: 0.0003096110493382236
D_B: 0.01092992938827187
G_B: 0.033556765510243246
cycle_B: 0.02729000454767161
idt_B: 0.0008692121441141296
End of epoch 10 / 200 	 
learning rate = 0.0002000
End of epoch 11 / 200 	 
learning rate = 0.0002000
End of epoch 12 / 200 	 
learning rate = 0.0002000
End of epoch 13 / 200 	 
learning rate = 0.0002000
End of epoch 14 / 200 	 
learning rate = 0.0002000
End of epoch 15 / 200 	 
learning rate = 0.0002000
End of epoch 16 / 200 	 
learning rate = 0.0002000
End of epoch 17 / 200 	 
learning rate = 0.0002000
End of epoch 18 / 200 	 
learning rate = 0.0002000
End of epoch 19 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 20, iters 324480
D_A: 0.016373893463730634
G_A: 0.027821318453537847
cycle_A: 0.005385880820045369
idt_A: 9.251415901329304e-05
D_B: 0.011094495988025236
G_B: 0.0364603112916475
cycle_B: 0.023690509172631758
idt_B: 0.0005078909358180358
End of epoch 20 / 200 	 
learning rate = 0.0002000
End of epoch 21 / 200 	 
learning rate = 0.0002000
End of epoch 22 / 200 	 
learning rate = 0.0002000
End of epoch 23 / 200 	 
learning rate = 0.0002000
End of epoch 24 / 200 	 
learning rate = 0.0002000
End of epoch 25 / 200 	 
learning rate = 0.0002000
End of epoch 26 / 200 	 
learning rate = 0.0002000
End of epoch 27 / 200 	 
learning rate = 0.0002000
End of epoch 28 / 200 	 
learning rate = 0.0002000
End of epoch 29 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 30, iters 486720
D_A: 0.010031102933779476
G_A: 0.028450549510812766
cycle_A: 0.009273669538473897
idt_A: 0.00011918369310143283
D_B: 0.007335857458020463
G_B: 0.042992963064810964
cycle_B: 0.018566547696436415
idt_B: 0.0004596623242228633
End of epoch 30 / 200 	 
learning rate = 0.0002000
End of epoch 31 / 200 	 
learning rate = 0.0002000
End of epoch 32 / 200 	 
learning rate = 0.0002000
End of epoch 33 / 200 	 
learning rate = 0.0002000
End of epoch 34 / 200 	 
learning rate = 0.0002000
End of epoch 35 / 200 	 
learning rate = 0.0002000
End of epoch 36 / 200 	 
learning rate = 0.0002000
End of epoch 37 / 200 	 
learning rate = 0.0002000
End of epoch 38 / 200 	 
learning rate = 0.0002000
End of epoch 39 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 40, iters 648960
D_A: 0.008707028621701153
G_A: 0.030646777793551985
cycle_A: 0.009275794657694885
idt_A: 0.00015120773680198126
D_B: 0.003034631603738868
G_B: 0.05441954112712742
cycle_B: 0.00731933081540494
idt_B: 0.0004659403091449693
End of epoch 40 / 200 	 
learning rate = 0.0002000
End of epoch 41 / 200 	 
learning rate = 0.0002000
End of epoch 42 / 200 	 
learning rate = 0.0002000
End of epoch 43 / 200 	 
learning rate = 0.0002000
End of epoch 44 / 200 	 
learning rate = 0.0002000
End of epoch 45 / 200 	 
learning rate = 0.0002000
End of epoch 46 / 200 	 
learning rate = 0.0002000
End of epoch 47 / 200 	 
learning rate = 0.0002000
End of epoch 48 / 200 	 
learning rate = 0.0002000
End of epoch 49 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 50, iters 811200
D_A: 0.014339493012424978
G_A: 0.02850045809174173
cycle_A: 0.007036576223124211
idt_A: 0.00011581127870199504
D_B: 0.002025922577062055
G_B: 0.05865365361218034
cycle_B: 0.002161673318202309
idt_B: 0.0002998630733129766
End of epoch 50 / 200 	 
learning rate = 0.0002000
End of epoch 51 / 200 	 
learning rate = 0.0002000
End of epoch 52 / 200 	 
learning rate = 0.0002000
End of epoch 53 / 200 	 
learning rate = 0.0002000
