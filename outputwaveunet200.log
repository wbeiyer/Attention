nohup: ignoring input
----------------- Options ---------------
               batch_size: 32                            	[default: 1]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 256                           
                 dataroot: ./datasets/thread             	[default: None]
             dataset_mode: aligned                       
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 10                            
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: lsgan                         
                  gpu_ids: 0,1,2,3,4,5,6,7               	[default: 0]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                lambda_L1: 50.0                          
              lambda_freq: 50.0                          
                load_iter: 0                             	[default: 0]
                load_size: 286                           
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: wave                          	[default: cycle_gan]
               n_layers_D: 3                             
                     name: thread_wave_unet              	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: unet_256                      
                      ngf: 64                            
                    niter: 100                           
              niter_decay: 100                           
               no_dropout: False                         
                  no_flip: False                         
                  no_html: False                         
                     norm: instance                      
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 50                            
               preprocess: resize_and_crop               
               print_freq: 100                           
                 saveDisk: False                         
             save_by_iter: False                         
          save_epoch_freq: 10                            
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                  verbose: False                         
----------------- End -------------------
dataset [AlignedDataset] was created
The number of training images = 16224
initialize network with normal
initialize network with normal
model [WaveModel] was created
---------- Networks initialized -------------
[Network G] Total number of parameters : 54.410 M
[Network D] Total number of parameters : 2.768 M
-----------------------------------------------
End of epoch 1 / 200 	 
learning rate = 0.0002000
End of epoch 2 / 200 	 
learning rate = 0.0002000
End of epoch 3 / 200 	 
learning rate = 0.0002000
End of epoch 4 / 200 	 
learning rate = 0.0002000
End of epoch 5 / 200 	 
learning rate = 0.0002000
End of epoch 6 / 200 	 
learning rate = 0.0002000
End of epoch 7 / 200 	 
learning rate = 0.0002000
End of epoch 8 / 200 	 
learning rate = 0.0002000
End of epoch 9 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 10, iters 162240
G_GAN: 0.029919152969174835
G_LL: 0.0069087221575588635
G_LH: 0.002168472490964101
G_HL: 0.002104849360343179
G_HH: 0.001097091470424387
G_L1: 0.1922277028450129
D_real: 0.0008766941934986098
D_fake: 0.0006298845485848176
End of epoch 10 / 200 	 
learning rate = 0.0002000
End of epoch 11 / 200 	 
learning rate = 0.0002000
End of epoch 12 / 200 	 
learning rate = 0.0002000
End of epoch 13 / 200 	 
learning rate = 0.0002000
End of epoch 14 / 200 	 
learning rate = 0.0002000
End of epoch 15 / 200 	 
learning rate = 0.0002000
End of epoch 16 / 200 	 
learning rate = 0.0002000
End of epoch 17 / 200 	 
learning rate = 0.0002000
End of epoch 18 / 200 	 
learning rate = 0.0002000
End of epoch 19 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 20, iters 324480
G_GAN: 0.030634473641889804
G_LL: 0.0067371032366077225
G_LH: 0.0021466108895948476
G_HL: 0.0020763826397150256
G_HH: 0.0010876909827839223
G_L1: 0.18809191588701815
D_real: 0.0004867383688297357
D_fake: 0.0004077008162120323
End of epoch 20 / 200 	 
learning rate = 0.0002000
End of epoch 21 / 200 	 
learning rate = 0.0002000
End of epoch 22 / 200 	 
learning rate = 0.0002000
End of epoch 23 / 200 	 
learning rate = 0.0002000
End of epoch 24 / 200 	 
learning rate = 0.0002000
End of epoch 25 / 200 	 
learning rate = 0.0002000
End of epoch 26 / 200 	 
learning rate = 0.0002000
End of epoch 27 / 200 	 
learning rate = 0.0002000
End of epoch 28 / 200 	 
learning rate = 0.0002000
End of epoch 29 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 30, iters 486720
G_GAN: 0.030133681728172115
G_LL: 0.006650854452261499
G_LH: 0.002141911744487268
G_HL: 0.00207534685746548
G_HH: 0.0010868059847819853
G_L1: 0.18599087981547593
D_real: 0.0004236506812323364
D_fake: 0.00034324757760715316
End of epoch 30 / 200 	 
learning rate = 0.0002000
End of epoch 31 / 200 	 
learning rate = 0.0002000
End of epoch 32 / 200 	 
learning rate = 0.0002000
End of epoch 33 / 200 	 
learning rate = 0.0002000
End of epoch 34 / 200 	 
learning rate = 0.0002000
End of epoch 35 / 200 	 
learning rate = 0.0002000
End of epoch 36 / 200 	 
learning rate = 0.0002000
End of epoch 37 / 200 	 
learning rate = 0.0002000
End of epoch 38 / 200 	 
learning rate = 0.0002000
End of epoch 39 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 40, iters 648960
G_GAN: 0.03073129415717821
G_LL: 0.006572165197639541
G_LH: 0.00213784937052464
G_HL: 0.0020681330316499815
G_HH: 0.001084931027047755
G_L1: 0.18421986593297246
D_real: 0.00028061775768448274
D_fake: 0.00026587370693762787
End of epoch 40 / 200 	 
learning rate = 0.0002000
End of epoch 41 / 200 	 
learning rate = 0.0002000
End of epoch 42 / 200 	 
learning rate = 0.0002000
End of epoch 43 / 200 	 
learning rate = 0.0002000
End of epoch 44 / 200 	 
learning rate = 0.0002000
End of epoch 45 / 200 	 
learning rate = 0.0002000
End of epoch 46 / 200 	 
learning rate = 0.0002000
End of epoch 47 / 200 	 
learning rate = 0.0002000
End of epoch 48 / 200 	 
learning rate = 0.0002000
End of epoch 49 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 50, iters 811200
G_GAN: 0.030940245432848998
G_LL: 0.006486471189476211
G_LH: 0.002131013578783052
G_HL: 0.0020620414621657754
G_HH: 0.0010833392119268415
G_L1: 0.18225181193864323
D_real: 0.00023529443463516796
D_fake: 0.0002191725251823497
End of epoch 50 / 200 	 
learning rate = 0.0002000
End of epoch 51 / 200 	 
learning rate = 0.0002000
End of epoch 52 / 200 	 
learning rate = 0.0002000
End of epoch 53 / 200 	 
learning rate = 0.0002000
End of epoch 54 / 200 	 
learning rate = 0.0002000
End of epoch 55 / 200 	 
learning rate = 0.0002000
End of epoch 56 / 200 	 
learning rate = 0.0002000
End of epoch 57 / 200 	 
learning rate = 0.0002000
End of epoch 58 / 200 	 
learning rate = 0.0002000
End of epoch 59 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 60, iters 973440
G_GAN: 0.030958980810507044
G_LL: 0.006408853130323236
G_LH: 0.0021293762328720395
G_HL: 0.0020597064128252116
G_HH: 0.0010835692614612686
G_L1: 0.18054151443922542
D_real: 0.000203993320137024
D_fake: 0.00019736335531441796
End of epoch 60 / 200 	 
learning rate = 0.0002000
End of epoch 61 / 200 	 
learning rate = 0.0002000
End of epoch 62 / 200 	 
learning rate = 0.0002000
End of epoch 63 / 200 	 
learning rate = 0.0002000
End of epoch 64 / 200 	 
learning rate = 0.0002000
End of epoch 65 / 200 	 
learning rate = 0.0002000
End of epoch 66 / 200 	 
learning rate = 0.0002000
End of epoch 67 / 200 	 
learning rate = 0.0002000
End of epoch 68 / 200 	 
learning rate = 0.0002000
End of epoch 69 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 70, iters 1135680
G_GAN: 0.031057083288564016
G_LL: 0.00635181091398853
G_LH: 0.0021285391931607583
G_HL: 0.002058434227106751
G_HH: 0.0010836655809038872
G_L1: 0.17927392071050535
D_real: 0.00016456041749273115
D_fake: 0.0001567988497357967
End of epoch 70 / 200 	 
learning rate = 0.0002000
End of epoch 71 / 200 	 
learning rate = 0.0002000
End of epoch 72 / 200 	 
learning rate = 0.0002000
End of epoch 73 / 200 	 
learning rate = 0.0002000
End of epoch 74 / 200 	 
learning rate = 0.0002000
End of epoch 75 / 200 	 
learning rate = 0.0002000
End of epoch 76 / 200 	 
learning rate = 0.0002000
End of epoch 77 / 200 	 
learning rate = 0.0002000
End of epoch 78 / 200 	 
learning rate = 0.0002000
End of epoch 79 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 80, iters 1297920
G_GAN: 0.031110538639995237
G_LL: 0.006284974772608021
G_LH: 0.0021258580334910745
G_HL: 0.0020551963303333675
G_HH: 0.0010827951329802502
G_L1: 0.17772868572489986
D_real: 0.00013924819543052077
D_fake: 0.0001431606190593492
End of epoch 80 / 200 	 
learning rate = 0.0002000
End of epoch 81 / 200 	 
learning rate = 0.0002000
End of epoch 82 / 200 	 
learning rate = 0.0002000
End of epoch 83 / 200 	 
learning rate = 0.0002000
End of epoch 84 / 200 	 
learning rate = 0.0002000
End of epoch 85 / 200 	 
learning rate = 0.0002000
End of epoch 86 / 200 	 
learning rate = 0.0002000
End of epoch 87 / 200 	 
learning rate = 0.0002000
End of epoch 88 / 200 	 
learning rate = 0.0002000
End of epoch 89 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 90, iters 1460160
G_GAN: 0.031152569099202665
G_LL: 0.006217769815547932
G_LH: 0.002125468833190141
G_HL: 0.002054781201332293
G_HH: 0.0010828839847358785
G_L1: 0.17620072668120706
D_real: 9.838288150948481e-05
D_fake: 0.00010831585815329241
End of epoch 90 / 200 	 
learning rate = 0.0002000
End of epoch 91 / 200 	 
learning rate = 0.0002000
End of epoch 92 / 200 	 
learning rate = 0.0002000
End of epoch 93 / 200 	 
learning rate = 0.0002000
End of epoch 94 / 200 	 
learning rate = 0.0002000
End of epoch 95 / 200 	 
learning rate = 0.0002000
End of epoch 96 / 200 	 
learning rate = 0.0002000
End of epoch 97 / 200 	 
learning rate = 0.0002000
End of epoch 98 / 200 	 
learning rate = 0.0002000
End of epoch 99 / 200 	 
learning rate = 0.0002000
saving the model at the end of epoch 100, iters 1622400
G_GAN: 0.03115364645389175
G_LL: 0.006141551749145197
G_LH: 0.0021219264577681113
G_HL: 0.0020532926948212044
G_HH: 0.00108232235772943
G_L1: 0.17438396173703835
D_real: 0.00012032853299162052
D_fake: 0.00011558823017789562
End of epoch 100 / 200 	 
learning rate = 0.0001980
End of epoch 101 / 200 	 
learning rate = 0.0001960
End of epoch 102 / 200 	 
learning rate = 0.0001941
End of epoch 103 / 200 	 
learning rate = 0.0001921
End of epoch 104 / 200 	 
learning rate = 0.0001901
End of epoch 105 / 200 	 
learning rate = 0.0001881
