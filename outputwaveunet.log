nohup: ignoring input
----------------- Options ---------------
               batch_size: 32                            	[default: 1]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 256                           
                 dataroot: ./datasets/thread             	[default: None]
             dataset_mode: aligned                       
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 10                            
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: lsgan                         
                  gpu_ids: 0,1,2,3,4,5,6,7               	[default: 0]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                lambda_L1: 5.0                           
              lambda_freq: 5.0                           
                load_iter: 0                             	[default: 0]
                load_size: 286                           
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: wave                          	[default: cycle_gan]
               n_layers_D: 3                             
                     name: thread_wave_unet              	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: unet_256                      
                      ngf: 64                            
                    niter: 60                            
              niter_decay: 60                            
               no_dropout: False                         
                  no_flip: False                         
                  no_html: False                         
                     norm: instance                      
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 50                            
               preprocess: resize_and_crop               
               print_freq: 100                           
                 saveDisk: False                         
             save_by_iter: False                         
          save_epoch_freq: 10                            
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                  verbose: False                         
----------------- End -------------------
dataset [AlignedDataset] was created
The number of training images = 16224
initialize network with normal
initialize network with normal
model [WaveModel] was created
---------- Networks initialized -------------
[Network G] Total number of parameters : 54.410 M
[Network D] Total number of parameters : 2.768 M
-----------------------------------------------
End of epoch 1 / 120 	 
learning rate = 0.0002000
End of epoch 2 / 120 	 
learning rate = 0.0002000
End of epoch 3 / 120 	 
learning rate = 0.0002000
End of epoch 4 / 120 	 
learning rate = 0.0002000
End of epoch 5 / 120 	 
learning rate = 0.0002000
End of epoch 6 / 120 	 
learning rate = 0.0002000
End of epoch 7 / 120 	 
learning rate = 0.0002000
End of epoch 8 / 120 	 
learning rate = 0.0002000
End of epoch 9 / 120 	 
learning rate = 0.0002000
saving the model at the end of epoch 10, iters 162240
G_GAN: 0.012548215394284893
G_LL: 0.008035281718078508
G_LH: 0.002777289735112905
G_HL: 0.0027335667583165612
G_HH: 0.001426779904312783
G_L1: 0.02249718674952697
D_real: 0.007443943500878836
D_fake: 0.006345372490908149
End of epoch 10 / 120 	 
learning rate = 0.0002000
End of epoch 11 / 120 	 
learning rate = 0.0002000
End of epoch 12 / 120 	 
learning rate = 0.0002000
End of epoch 13 / 120 	 
learning rate = 0.0002000
End of epoch 14 / 120 	 
learning rate = 0.0002000
End of epoch 15 / 120 	 
learning rate = 0.0002000
End of epoch 16 / 120 	 
learning rate = 0.0002000
End of epoch 17 / 120 	 
learning rate = 0.0002000
End of epoch 18 / 120 	 
learning rate = 0.0002000
End of epoch 19 / 120 	 
learning rate = 0.0002000
saving the model at the end of epoch 20, iters 324480
G_GAN: 0.01360342650687142
G_LL: 0.008211496122607582
G_LH: 0.0029415768390719794
G_HL: 0.0028412195376746902
G_HH: 0.001514620675991934
G_L1: 0.023038294206738943
D_real: 0.007171071547357259
D_fake: 0.006029817762488915
End of epoch 20 / 120 	 
learning rate = 0.0002000
End of epoch 21 / 120 	 
learning rate = 0.0002000
End of epoch 22 / 120 	 
learning rate = 0.0002000
End of epoch 23 / 120 	 
learning rate = 0.0002000
End of epoch 24 / 120 	 
learning rate = 0.0002000
End of epoch 25 / 120 	 
learning rate = 0.0002000
End of epoch 26 / 120 	 
learning rate = 0.0002000
End of epoch 27 / 120 	 
learning rate = 0.0002000
End of epoch 28 / 120 	 
learning rate = 0.0002000
End of epoch 29 / 120 	 
learning rate = 0.0002000
saving the model at the end of epoch 30, iters 486720
G_GAN: 0.013851439054041395
G_LL: 0.008213883091344753
G_LH: 0.0029694749431820548
G_HL: 0.002881244282813144
G_HH: 0.0015324887411330903
G_L1: 0.023080099318948017
D_real: 0.007097289032997156
D_fake: 0.005948683707809528
End of epoch 30 / 120 	 
learning rate = 0.0002000
End of epoch 31 / 120 	 
learning rate = 0.0002000
End of epoch 32 / 120 	 
learning rate = 0.0002000
End of epoch 33 / 120 	 
learning rate = 0.0002000
End of epoch 34 / 120 	 
learning rate = 0.0002000
End of epoch 35 / 120 	 
learning rate = 0.0002000
End of epoch 36 / 120 	 
learning rate = 0.0002000
End of epoch 37 / 120 	 
learning rate = 0.0002000
End of epoch 38 / 120 	 
learning rate = 0.0002000
End of epoch 39 / 120 	 
learning rate = 0.0002000
saving the model at the end of epoch 40, iters 648960
G_GAN: 0.013425156646784711
G_LL: 0.008053088948292257
G_LH: 0.0029536196048127183
G_HL: 0.0028410036970091582
G_HH: 0.001525526398474156
G_L1: 0.02267720641777713
D_real: 0.007237186582843521
D_fake: 0.006095623589695764
End of epoch 40 / 120 	 
learning rate = 0.0002000
End of epoch 41 / 120 	 
learning rate = 0.0002000
End of epoch 42 / 120 	 
learning rate = 0.0002000
End of epoch 43 / 120 	 
learning rate = 0.0002000
End of epoch 44 / 120 	 
learning rate = 0.0002000
End of epoch 45 / 120 	 
learning rate = 0.0002000
End of epoch 46 / 120 	 
learning rate = 0.0002000
End of epoch 47 / 120 	 
learning rate = 0.0002000
End of epoch 48 / 120 	 
learning rate = 0.0002000
End of epoch 49 / 120 	 
learning rate = 0.0002000
saving the model at the end of epoch 50, iters 811200
G_GAN: 0.013790478733785201
G_LL: 0.008118574326597081
G_LH: 0.003010575003330495
G_HL: 0.002911138993502398
G_HH: 0.0015625202327073943
G_L1: 0.022877026779147294
D_real: 0.006935805871741042
D_fake: 0.005721636972742805
End of epoch 50 / 120 	 
learning rate = 0.0002000
End of epoch 51 / 120 	 
learning rate = 0.0002000
End of epoch 52 / 120 	 
learning rate = 0.0002000
End of epoch 53 / 120 	 
learning rate = 0.0002000
End of epoch 54 / 120 	 
learning rate = 0.0002000
End of epoch 55 / 120 	 
learning rate = 0.0002000
End of epoch 56 / 120 	 
learning rate = 0.0002000
End of epoch 57 / 120 	 
learning rate = 0.0002000
End of epoch 58 / 120 	 
learning rate = 0.0002000
End of epoch 59 / 120 	 
learning rate = 0.0002000
saving the model at the end of epoch 60, iters 973440
G_GAN: 0.014509646816700843
G_LL: 0.008107372711153601
G_LH: 0.003057208068804775
G_HL: 0.0029396213512110875
G_HH: 0.0016029910356655012
G_L1: 0.022899520930630215
D_real: 0.006533830032382653
D_fake: 0.005332251107623282
End of epoch 60 / 120 	 
learning rate = 0.0001967
End of epoch 61 / 120 	 
learning rate = 0.0001934
End of epoch 62 / 120 	 
learning rate = 0.0001902
End of epoch 63 / 120 	 
learning rate = 0.0001869
End of epoch 64 / 120 	 
learning rate = 0.0001836
End of epoch 65 / 120 	 
learning rate = 0.0001803
End of epoch 66 / 120 	 
learning rate = 0.0001770
End of epoch 67 / 120 	 
learning rate = 0.0001738
End of epoch 68 / 120 	 
learning rate = 0.0001705
End of epoch 69 / 120 	 
learning rate = 0.0001672
saving the model at the end of epoch 70, iters 1135680
G_GAN: 0.016429175935498266
G_LL: 0.008049716965407428
G_LH: 0.0030680812589449465
G_HL: 0.0029797857940468855
G_HH: 0.0016539927429190784
G_L1: 0.02279831519390822
D_real: 0.005833139801737751
D_fake: 0.004689035477039553
End of epoch 70 / 120 	 
learning rate = 0.0001639
End of epoch 71 / 120 	 
learning rate = 0.0001607
End of epoch 72 / 120 	 
learning rate = 0.0001574
End of epoch 73 / 120 	 
learning rate = 0.0001541
End of epoch 74 / 120 	 
learning rate = 0.0001508
End of epoch 75 / 120 	 
learning rate = 0.0001475
End of epoch 76 / 120 	 
learning rate = 0.0001443
End of epoch 77 / 120 	 
learning rate = 0.0001410
End of epoch 78 / 120 	 
learning rate = 0.0001377
End of epoch 79 / 120 	 
learning rate = 0.0001344
saving the model at the end of epoch 80, iters 1297920
G_GAN: 0.018506680260719163
G_LL: 0.008029568503212176
G_LH: 0.0030768827895249957
G_HL: 0.003000718744883141
G_HH: 0.00169570061940281
G_L1: 0.022774022612610512
D_real: 0.004850265724975726
D_fake: 0.003879255790831025
End of epoch 80 / 120 	 
learning rate = 0.0001311
End of epoch 81 / 120 	 
learning rate = 0.0001279
End of epoch 82 / 120 	 
learning rate = 0.0001246
End of epoch 83 / 120 	 
learning rate = 0.0001213
End of epoch 84 / 120 	 
learning rate = 0.0001180
End of epoch 85 / 120 	 
learning rate = 0.0001148
End of epoch 86 / 120 	 
learning rate = 0.0001115
End of epoch 87 / 120 	 
learning rate = 0.0001082
End of epoch 88 / 120 	 
learning rate = 0.0001049
End of epoch 89 / 120 	 
learning rate = 0.0001016
saving the model at the end of epoch 90, iters 1460160
G_GAN: 0.021758151593337046
G_LL: 0.007990919844352106
G_LH: 0.003067537956215309
G_HL: 0.0030023241585535702
G_HH: 0.0017095691055110866
G_L1: 0.02268484931373032
D_real: 0.0036592418169492536
D_fake: 0.0028851680572170406
End of epoch 90 / 120 	 
learning rate = 0.0000984
End of epoch 91 / 120 	 
learning rate = 0.0000951
End of epoch 92 / 120 	 
learning rate = 0.0000918
End of epoch 93 / 120 	 
learning rate = 0.0000885
End of epoch 94 / 120 	 
learning rate = 0.0000852
End of epoch 95 / 120 	 
learning rate = 0.0000820
End of epoch 96 / 120 	 
learning rate = 0.0000787
End of epoch 97 / 120 	 
learning rate = 0.0000754
End of epoch 98 / 120 	 
learning rate = 0.0000721
End of epoch 99 / 120 	 
learning rate = 0.0000689
saving the model at the end of epoch 100, iters 1622400
G_GAN: 0.024593171801351937
G_LL: 0.007936561164212885
G_LH: 0.0030485344940476868
G_HL: 0.0029923879588298367
G_HH: 0.0017062482987558083
G_L1: 0.022526514578676788
D_real: 0.002593793010539366
D_fake: 0.0020511321106023673
End of epoch 100 / 120 	 
learning rate = 0.0000656
End of epoch 101 / 120 	 
learning rate = 0.0000623
End of epoch 102 / 120 	 
learning rate = 0.0000590
End of epoch 103 / 120 	 
learning rate = 0.0000557
End of epoch 104 / 120 	 
learning rate = 0.0000525
End of epoch 105 / 120 	 
learning rate = 0.0000492
End of epoch 106 / 120 	 
learning rate = 0.0000459
End of epoch 107 / 120 	 
learning rate = 0.0000426
End of epoch 108 / 120 	 
learning rate = 0.0000393
End of epoch 109 / 120 	 
learning rate = 0.0000361
saving the model at the end of epoch 110, iters 1784640
G_GAN: 0.02598600043259429
G_LL: 0.007879430405185715
G_LH: 0.0030428654848574065
G_HL: 0.0029839237080060343
G_HH: 0.0017021324998043345
G_L1: 0.022373263711289778
D_real: 0.001925090933471176
D_fake: 0.0014998498409472157
End of epoch 110 / 120 	 
learning rate = 0.0000328
End of epoch 111 / 120 	 
learning rate = 0.0000295
End of epoch 112 / 120 	 
learning rate = 0.0000262
End of epoch 113 / 120 	 
learning rate = 0.0000230
End of epoch 114 / 120 	 
learning rate = 0.0000197
End of epoch 115 / 120 	 
learning rate = 0.0000164
End of epoch 116 / 120 	 
learning rate = 0.0000131
End of epoch 117 / 120 	 
learning rate = 0.0000098
End of epoch 118 / 120 	 
learning rate = 0.0000066
End of epoch 119 / 120 	 
learning rate = 0.0000033
saving the model at the end of epoch 120, iters 1946880
G_GAN: 0.026740975861659886
G_LL: 0.007834140353705697
G_LH: 0.003018294861062597
G_HL: 0.002964222721968588
G_HH: 0.001687504507737194
G_L1: 0.022232123365886584
D_real: 0.0015651556043022293
D_fake: 0.0011875025251342067
End of epoch 120 / 120 	 
learning rate = 0.0000000
Training losses saved to training_losses.xlsx
